# LIME Explainability for Machine Learning Models

## Overview
- This assignment demonstrates how to use LIME (Local Interpretable Model-agnostic Explanations) to explain a Random Forest Classifier trained on the Iris dataset. 
- LIME helps us understand which features contribute the most to a model's prediction.

## Steps Covered
1. **Load the Iris dataset** - A dataset with 150 samples categorized into three species.
2. **Train a Random Forest Classifier** - A machine learning model used for classification.
3. **Use LIME for Explainability** - Explain how the model makes predictions for a given instance.
4. **Visualize Feature Importance** - Generate bar plots to highlight key contributing features.


## Visualization
- **Feature Contribution Plot:** Shows which features had the strongest influence on the modelâ€™s decision.

## Most Influential Features
- Features with high positive contribution increase the probability of the predicted class.
- Features with high negative contribution push the model away from that prediction.

## Installation and Setup

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/johnjoel2001/Human_AI_Interaction.git
   cd Human_AI_Interaction
   ```
2. Run the Human-AI-Interaction.ipynb
3. The requirements are listed in the first cell.

## Note

AI Tools were not used in this Assignment
